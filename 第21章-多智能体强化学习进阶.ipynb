{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6289,
     "status": "ok",
     "timestamp": 1649958316791,
     "user": {
      "displayName": "Sam Lu",
      "userId": "15789059763790170725"
     },
     "user_tz": -480
    },
    "id": "NK5O2SUrK_xL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import rl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17090,
     "status": "ok",
     "timestamp": 1649958333876,
     "user": {
      "displayName": "Sam Lu",
      "userId": "15789059763790170725"
     },
     "user_tz": -480
    },
    "id": "2pfzwJiJK_xO",
    "outputId": "e7050004-2701-4bec-f531-16774f1eb026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///D:/%E5%A4%A7%E4%B8%89%E4%B8%8B/%E7%8A%80%E7%89%9B%E9%B8%9F/Hands-on-RL/multiagent-particle-envs\n",
      "Requirement already satisfied: gym in d:\\python\\setup\\lib\\site-packages (from multiagent==0.0.1) (0.23.1)\n",
      "Collecting numpy-stl\n",
      "  Downloading numpy_stl-2.17.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in d:\\python\\setup\\lib\\site-packages (from gym->multiagent==0.0.1) (1.22.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in d:\\python\\setup\\lib\\site-packages (from gym->multiagent==0.0.1) (4.11.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\python\\setup\\lib\\site-packages (from gym->multiagent==0.0.1) (1.3.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in d:\\python\\setup\\lib\\site-packages (from gym->multiagent==0.0.1) (0.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\python\\setup\\lib\\site-packages (from importlib-metadata>=4.10.0->gym->multiagent==0.0.1) (3.8.0)\n",
      "Collecting python-utils>=1.6.2\n",
      "  Downloading python_utils-3.3.2-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: python-utils, numpy-stl, multiagent\n",
      "  Running setup.py develop for multiagent\n",
      "Successfully installed multiagent-0.0.1 numpy-stl-2.17.1 python-utils-3.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "  WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "  WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the 'd:\\python\\setup\\python.exe -m pip install --upgrade pip' command.\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pettingzoo 1.18.1 requires gym>=0.21.0, but you have gym 0.10.5 which is incompatible.\n",
      "tianshou 0.4.8 requires gym>=0.15.4, but you have gym 0.10.5 which is incompatible.\n",
      "learn2learn 0.1.5 requires gym>=0.14.0, but you have gym 0.10.5 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (d:\\python\\setup\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the 'd:\\python\\setup\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/boyu-ai/multiagent-particle-envs.git --quiet\n",
    "!pip install -e multiagent-particle-envs\n",
    "import sys\n",
    "sys.path.append(\"multiagent-particle-envs\")\n",
    "# 由于multiagent-pariticle-env底层的实现有一些版本问题,因此gym需要改为可用的版本\n",
    "!pip install --upgrade gym==0.10.5 -q\n",
    "import gym\n",
    "from multiagent.environment import MultiAgentEnv\n",
    "import multiagent.scenarios as scenarios\n",
    "\n",
    "\n",
    "def make_env(scenario_name):\n",
    "    # 从环境文件脚本中创建环境\n",
    "    scenario = scenarios.load(scenario_name + \".py\").Scenario()\n",
    "    world = scenario.make_world()\n",
    "    env = MultiAgentEnv(world, scenario.reset_world, scenario.reward,\n",
    "                        scenario.observation)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1649958333876,
     "user": {
      "displayName": "Sam Lu",
      "userId": "15789059763790170725"
     },
     "user_tz": -480
    },
    "id": "-dpzMRpWK_xP"
   },
   "outputs": [],
   "source": [
    "def onehot_from_logits(logits, eps=0.01):\n",
    "    ''' 生成最优动作的独热（one-hot）形式 '''\n",
    "    argmax_acs = (logits == logits.max(1, keepdim=True)[0]).float()\n",
    "    # 生成随机动作,转换成独热形式\n",
    "    rand_acs = torch.autograd.Variable(torch.eye(logits.shape[1])[[\n",
    "        np.random.choice(range(logits.shape[1]), size=logits.shape[0])\n",
    "    ]],\n",
    "                                       requires_grad=False).to(logits.device)\n",
    "    # 通过epsilon-贪婪算法来选择用哪个动作\n",
    "    return torch.stack([\n",
    "        argmax_acs[i] if r > eps else rand_acs[i]\n",
    "        for i, r in enumerate(torch.rand(logits.shape[0]))\n",
    "    ])\n",
    "\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-20, tens_type=torch.FloatTensor):\n",
    "    \"\"\"从Gumbel(0,1)分布中采样\"\"\"\n",
    "    U = torch.autograd.Variable(tens_type(*shape).uniform_(),\n",
    "                                requires_grad=False)\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    \"\"\" 从Gumbel-Softmax分布中采样\"\"\"\n",
    "    y = logits + sample_gumbel(logits.shape, tens_type=type(logits.data)).to(\n",
    "        logits.device)\n",
    "    return F.softmax(y / temperature, dim=1)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, temperature=1.0):\n",
    "    \"\"\"从Gumbel-Softmax分布中采样,并进行离散化\"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    y_hard = onehot_from_logits(y)\n",
    "    y = (y_hard.to(logits.device) - y).detach() + y\n",
    "    # 返回一个y_hard的独热量,但是它的梯度是y,我们既能够得到一个与环境交互的离散动作,又可以\n",
    "    # 正确地反传梯度\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1649958333877,
     "user": {
      "displayName": "Sam Lu",
      "userId": "15789059763790170725"
     },
     "user_tz": -480
    },
    "id": "RPIC_vPUK_xQ"
   },
   "outputs": [],
   "source": [
    "class TwoLayerFC(torch.nn.Module):\n",
    "    def __init__(self, num_in, num_out, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(num_in, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, num_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    ''' DDPG算法 '''\n",
    "    def __init__(self, state_dim, action_dim, critic_input_dim, hidden_dim,\n",
    "                 actor_lr, critic_lr, device):\n",
    "        self.actor = TwoLayerFC(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_actor = TwoLayerFC(state_dim, action_dim,\n",
    "                                       hidden_dim).to(device)\n",
    "        self.critic = TwoLayerFC(critic_input_dim, 1, hidden_dim).to(device)\n",
    "        self.target_critic = TwoLayerFC(critic_input_dim, 1,\n",
    "                                        hidden_dim).to(device)\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n",
    "                                                 lr=critic_lr)\n",
    "\n",
    "    def take_action(self, state, explore=False):\n",
    "        action = self.actor(state)\n",
    "        if explore:\n",
    "            action = gumbel_softmax(action)\n",
    "        else:\n",
    "            action = onehot_from_logits(action)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "\n",
    "    def soft_update(self, net, target_net, tau):\n",
    "        for param_target, param in zip(target_net.parameters(),\n",
    "                                       net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - tau) +\n",
    "                                    param.data * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1649958333877,
     "user": {
      "displayName": "Sam Lu",
      "userId": "15789059763790170725"
     },
     "user_tz": -480
    },
    "id": "UgJlmdo2K_xR"
   },
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "    def __init__(self, env, device, actor_lr, critic_lr, hidden_dim,\n",
    "                 state_dims, action_dims, critic_input_dim, gamma, tau):\n",
    "        self.agents = []\n",
    "        for i in range(len(env.agents)):\n",
    "            self.agents.append(\n",
    "                DDPG(state_dims[i], action_dims[i], critic_input_dim,\n",
    "                     hidden_dim, actor_lr, critic_lr, device))\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.critic_criterion = torch.nn.MSELoss()\n",
    "        self.device = device\n",
    "\n",
    "    @property\n",
    "    def policies(self):\n",
    "        return [agt.actor for agt in self.agents]\n",
    "\n",
    "    @property\n",
    "    def target_policies(self):\n",
    "        return [agt.target_actor for agt in self.agents]\n",
    "\n",
    "    def take_action(self, states, explore):\n",
    "        states = [\n",
    "            torch.tensor([states[i]], dtype=torch.float, device=self.device)\n",
    "            for i in range(len(env.agents))\n",
    "        ]\n",
    "        return [\n",
    "            agent.take_action(state, explore)\n",
    "            for agent, state in zip(self.agents, states)\n",
    "        ]\n",
    "\n",
    "    def update(self, sample, i_agent):\n",
    "        obs, act, rew, next_obs, done = sample\n",
    "        cur_agent = self.agents[i_agent]\n",
    "\n",
    "        cur_agent.critic_optimizer.zero_grad()\n",
    "        all_target_act = [\n",
    "            onehot_from_logits(pi(_next_obs))\n",
    "            for pi, _next_obs in zip(self.target_policies, next_obs)\n",
    "        ]\n",
    "        target_critic_input = torch.cat((*next_obs, *all_target_act), dim=1)\n",
    "        target_critic_value = rew[i_agent].view(\n",
    "            -1, 1) + self.gamma * cur_agent.target_critic(\n",
    "                target_critic_input) * (1 - done[i_agent].view(-1, 1))\n",
    "        critic_input = torch.cat((*obs, *act), dim=1)\n",
    "        critic_value = cur_agent.critic(critic_input)\n",
    "        critic_loss = self.critic_criterion(critic_value,\n",
    "                                            target_critic_value.detach())\n",
    "        critic_loss.backward()\n",
    "        cur_agent.critic_optimizer.step()\n",
    "\n",
    "        cur_agent.actor_optimizer.zero_grad()\n",
    "        cur_actor_out = cur_agent.actor(obs[i_agent])\n",
    "        cur_act_vf_in = gumbel_softmax(cur_actor_out)\n",
    "        all_actor_acs = []\n",
    "        for i, (pi, _obs) in enumerate(zip(self.policies, obs)):\n",
    "            if i == i_agent:\n",
    "                all_actor_acs.append(cur_act_vf_in)\n",
    "            else:\n",
    "                all_actor_acs.append(onehot_from_logits(pi(_obs)))\n",
    "        vf_in = torch.cat((*obs, *all_actor_acs), dim=1)\n",
    "        actor_loss = -cur_agent.critic(vf_in).mean()\n",
    "        actor_loss += (cur_actor_out**2).mean() * 1e-3\n",
    "        actor_loss.backward()\n",
    "        cur_agent.actor_optimizer.step()\n",
    "\n",
    "    def update_all_targets(self):\n",
    "        '''\n",
    "        更新目标网络\n",
    "        '''\n",
    "        for agt in self.agents:\n",
    "            agt.soft_update(agt.actor, agt.target_actor, self.tau)\n",
    "            agt.soft_update(agt.critic, agt.target_critic, self.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1649958334400,
     "user": {
      "displayName": "Sam Lu",
      "userId": "15789059763790170725"
     },
     "user_tz": -480
    },
    "id": "HpnsWpsFK_xS"
   },
   "outputs": [],
   "source": [
    "num_episodes = 5000\n",
    "episode_length = 25  # 每条序列的最大长度\n",
    "buffer_size = 100000\n",
    "hidden_dim = 64\n",
    "actor_lr = 1e-2\n",
    "critic_lr = 1e-2\n",
    "gamma = 0.95\n",
    "tau = 1e-2\n",
    "batch_size = 1024\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "update_interval = 100\n",
    "minimal_size = 4000\n",
    "\n",
    "env_id = \"simple_adversary\"\n",
    "env = make_env(env_id)\n",
    "replay_buffer = rl_utils.ReplayBuffer(buffer_size)\n",
    "\n",
    "state_dims = []\n",
    "action_dims = []\n",
    "for action_space in env.action_space:\n",
    "    action_dims.append(action_space.n)\n",
    "for state_space in env.observation_space:\n",
    "    state_dims.append(state_space.shape[0])\n",
    "critic_input_dim = sum(state_dims) + sum(action_dims)\n",
    "\n",
    "maddpg = MADDPG(env, device, actor_lr, critic_lr, hidden_dim, state_dims,\n",
    "                action_dims, critic_input_dim, gamma, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 639428,
     "status": "ok",
     "timestamp": 1649958973823,
     "user": {
      "displayName": "Sam Lu",
      "userId": "15789059763790170725"
     },
     "user_tz": -480
    },
    "id": "oJYndDnWK_xS",
    "outputId": "e2e0b3de-b9b7-45ee-ef3b-84786c9dfa58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1024\n",
      "3\n",
      "3\n",
      "1024\n",
      "3\n",
      "1024\n",
      "3\n",
      "1024\n",
      "3\n",
      "1024\n",
      "3\n",
      "1024\n",
      "5\n",
      "1024\n",
      "3\n",
      "3\n",
      "1024\n",
      "3\n",
      "1024\n",
      "3\n",
      "1024\n",
      "3\n",
      "1024\n",
      "3\n",
      "1024\n",
      "5\n",
      "1024\n",
      "3\n",
      "3\n",
      "1024\n",
      "3\n",
      "1024\n",
      "3\n",
      "1024\n",
      "3\n",
      "1024\n",
      "3\n",
      "1024\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e1eedf56d46a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0ma_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mmaddpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[0mmaddpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_all_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi_episode\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-738bb1666780>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, sample, i_agent)\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[0mall_actor_acs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_act_vf_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                 \u001b[0mall_actor_acs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0monehot_from_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_obs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mvf_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mall_actor_acs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mactor_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mcur_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvf_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a40226ba3f94>\u001b[0m in \u001b[0;36monehot_from_logits\u001b[1;34m(logits, eps)\u001b[0m\n\u001b[0;32m      8\u001b[0m                                        requires_grad=False).to(logits.device)\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# 通过epsilon-贪婪算法来选择用哪个动作\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     return torch.stack([\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0margmax_acs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0meps\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrand_acs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate(env_id, maddpg, n_episode=10, episode_length=25):\n",
    "    # 对学习的策略进行评估,此时不会进行探索\n",
    "    env = make_env(env_id)\n",
    "    returns = np.zeros(len(env.agents))\n",
    "    for _ in range(n_episode):\n",
    "        obs = env.reset()\n",
    "        for t_i in range(episode_length):\n",
    "            actions = maddpg.take_action(obs, explore=False)\n",
    "            obs, rew, done, info = env.step(actions)\n",
    "            rew = np.array(rew)\n",
    "            returns += rew / n_episode\n",
    "    return returns.tolist()\n",
    "\n",
    "\n",
    "return_list = []  # 记录每一轮的回报（return）\n",
    "total_step = 0\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    # ep_returns = np.zeros(len(env.agents))\n",
    "    for e_i in range(episode_length):\n",
    "        actions = maddpg.take_action(state, explore=True)\n",
    "        next_state, reward, done, _ = env.step(actions)\n",
    "        \n",
    "        replay_buffer.add(state, actions, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        total_step += 1\n",
    "        if replay_buffer.size(\n",
    "        ) >= minimal_size and total_step % update_interval == 0:\n",
    "            sample = replay_buffer.sample(batch_size)\n",
    "#             print(len(sample))\n",
    "#             print(len(sample[0]))\n",
    "#             print(len(sample[0][0]))\n",
    "            \n",
    "            '''\n",
    "            5, batchsize , numbers\n",
    "            '''\n",
    "            def stack_array(x):\n",
    "                rearranged = [[sub_x[i] for sub_x in x]\n",
    "                              for i in range(len(x[0]))]\n",
    "                \n",
    "#                 print(len(rearranged))\n",
    "#                 print(len(rearranged[0]))\n",
    "                '''\n",
    "                id ， batch size\n",
    "                '''\n",
    "                return [\n",
    "                    torch.FloatTensor(np.vstack(aa)).to(device)\n",
    "                    for aa in rearranged\n",
    "                ]\n",
    "\n",
    "            sample = [stack_array(x) for x in sample]\n",
    "\n",
    "            \n",
    "            for a_i in range(len(env.agents)):\n",
    "                maddpg.update(sample, a_i)\n",
    "            maddpg.update_all_targets()\n",
    "    if (i_episode + 1) % 100 == 0:\n",
    "        ep_returns = evaluate(env_id, maddpg, n_episode=100)\n",
    "        return_list.append(ep_returns)\n",
    "        print(f\"Episode: {i_episode+1}, {ep_returns}\")\n",
    "\n",
    "# Episode: 100, [-139.85078880125366, 24.84409588589504, 24.84409588589504]\n",
    "\n",
    "# /content/rl_utils.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged\n",
    "# nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different\n",
    "# lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=\n",
    "# object' when creating the ndarray\n",
    "#   return np.array(state), action, reward, np.array(next_state), done\n",
    "\n",
    "# Episode: 200, [-105.11447331630691, -4.667816632926483, -4.667816632926483]\n",
    "# Episode: 300, [-31.04371751870054, 2.367667721218739, 2.367667721218739]\n",
    "# Episode: 400, [-25.856803405338162, -1.6019954659169862, -1.6019954659169862]\n",
    "# Episode: 500, [-14.863629584466256, -6.493559215483058, -6.493559215483058]\n",
    "# Episode: 600, [-11.753253499724337, 1.1278364537452759, 1.1278364537452759]\n",
    "# Episode: 700, [-12.55948132966949, 0.36995365890528387, 0.36995365890528387]\n",
    "# Episode: 800, [-11.204469505024559, 5.799833097835371, 5.799833097835371]\n",
    "# Episode: 900, [-12.793323601010943, 7.0357387891514716, 7.0357387891514716]\n",
    "# Episode: 1000, [-9.731828562147946, 5.203205531782827, 5.203205531782827]\n",
    "# Episode: 1100, [-8.510131349426718, 5.2461119857635135, 5.2461119857635135]\n",
    "# Episode: 1200, [-9.585692738161287, 6.777259476592237, 6.777259476592237]\n",
    "# Episode: 1300, [-9.826005870972006, 7.207743730178556, 7.207743730178556]\n",
    "# Episode: 1400, [-8.566589499183216, 6.2620796176791, 6.2620796176791]\n",
    "# Episode: 1500, [-8.543261572521422, 5.8545569515458755, 5.8545569515458755]\n",
    "# Episode: 1600, [-9.719611039111387, 6.136607469223544, 6.136607469223544]\n",
    "# Episode: 1700, [-8.2925932025312, 5.435361693227948, 5.435361693227948]\n",
    "# Episode: 1800, [-8.959067279108076, 5.990426636679429, 5.990426636679429]\n",
    "# Episode: 1900, [-8.8242500783286, 5.307928537097473, 5.307928537097473]\n",
    "# Episode: 2000, [-8.20281209652912, 5.689542567717828, 5.689542567717828]\n",
    "# Episode: 2100, [-9.04772055064216, 5.583820408577938, 5.583820408577938]\n",
    "# Episode: 2200, [-8.50059251561189, 5.6745737134871215, 5.6745737134871215]\n",
    "# Episode: 2300, [-6.878826441166284, 4.451387010062865, 4.451387010062865]\n",
    "# Episode: 2400, [-9.324710297045764, 5.414272587118738, 5.414272587118738]\n",
    "# Episode: 2500, [-8.215515333155677, 5.0714473072251085, 5.0714473072251085]\n",
    "# Episode: 2600, [-9.710948754211286, 5.945957102784014, 5.945957102784014]\n",
    "# Episode: 2700, [-6.95987837179912, 4.306175766599912, 4.306175766599912]\n",
    "# Episode: 2800, [-7.69945047297023, 4.63572107199487, 4.63572107199487]\n",
    "# Episode: 2900, [-7.640228784974167, 5.129701244255248, 5.129701244255248]\n",
    "# Episode: 3000, [-7.33452401443051, 4.234568124813538, 4.234568124813538]\n",
    "# Episode: 3100, [-7.561209771041727, 4.551318252296591, 4.551318252296591]\n",
    "# Episode: 3200, [-7.303825192093116, 4.1751459368803525, 4.1751459368803525]\n",
    "# Episode: 3300, [-7.4085041799390225, 4.324439976487989, 4.324439976487989]\n",
    "# Episode: 3400, [-8.831540597437234, 5.095912768930884, 5.095912768930884]\n",
    "# Episode: 3500, [-7.909255169344246, 4.814617328955552, 4.814617328955552]\n",
    "# Episode: 3600, [-8.102049625513107, 4.218137021221713, 4.218137021221713]\n",
    "# Episode: 3700, [-7.124044426425797, 4.22171591046473, 4.22171591046473]\n",
    "# Episode: 3800, [-9.855226095181644, 5.559444947358021, 5.559444947358021]\n",
    "# Episode: 3900, [-8.112882872673746, 4.601425710926074, 4.601425710926074]\n",
    "# Episode: 4000, [-7.7353843779903855, 4.842239161334104, 4.842239161334104]\n",
    "# Episode: 4100, [-7.877527887061531, 4.593953921896876, 4.593953921896876]\n",
    "# Episode: 4200, [-7.401751185392445, 4.52101055148277, 4.52101055148277]\n",
    "# Episode: 4300, [-8.233404140017905, 4.713286609882572, 4.713286609882572]\n",
    "# Episode: 4400, [-8.653939326472079, 5.184954272702421, 5.184954272702421]\n",
    "# Episode: 4500, [-9.767723118921353, 6.570082634111054, 6.570082634111054]\n",
    "# Episode: 4600, [-9.30060260689829, 5.242836047978754, 5.242836047978754]\n",
    "# Episode: 4700, [-8.964009029648428, 4.901113456984634, 4.901113456984634]\n",
    "# Episode: 4800, [-10.22982114177131, 5.669039384469422, 5.669039384469422]\n",
    "# Episode: 4900, [-10.568961308877448, 4.479337463298422, 4.479337463298422]\n",
    "# Episode: 5000, [-8.700993807143094, 4.4632810497979705, 4.4632810497979705]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "executionInfo": {
     "elapsed": 705,
     "status": "ok",
     "timestamp": 1649958974522,
     "user": {
      "displayName": "Sam Lu",
      "userId": "15789059763790170725"
     },
     "user_tz": -480
    },
    "id": "wL6fHge0K_xT",
    "outputId": "dc2d1d95-a178-4bde-a06c-e736c447d0bc"
   },
   "outputs": [],
   "source": [
    "return_array = np.array(return_list)\n",
    "for i, agent_name in enumerate([\"adversary_0\", \"agent_0\", \"agent_1\"]):\n",
    "    plt.figure()\n",
    "    plt.plot(\n",
    "        np.arange(return_array.shape[0]) * 100,\n",
    "        rl_utils.moving_average(return_array[:, i], 9))\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Returns\")\n",
    "    plt.title(f\"{agent_name} by MADDPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第21章-多智能体强化学习进阶.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
